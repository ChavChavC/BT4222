{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NVSXuBUXmDW1",
        "YtXO1BHVIRMg"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLQMDI7tVv9xEpCvLbrzBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChavChavC/BT4222/blob/main/FE_NGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pY8XXavXg7H0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4813dc-82ef-495a-b0ce-81c5920f204c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.19.0 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NGRAM"
      ],
      "metadata": {
        "id": "NVSXuBUXmDW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('try_cleaned_data.csv')\n",
        "df = df.drop(columns=df.columns[0], axis=1)\n",
        "df[df.isnull().any(axis=1)]\n",
        "df.dropna(subset=['title'], inplace=True)\n",
        "data= df['title'].tolist()\n",
        "labels_df = df[['labels']]\n",
        "labels_array = df['labels'].tolist()"
      ],
      "metadata": {
        "id": "M5xagF2qhU7-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for foreign characters\n",
        "df.loc[df['title'].str.contains('気候の非常事態への対策として大幅な炭素削減を可能にするモバイル技術')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "EcYlNtqUMDRj",
        "outputId": "8a074ce3-e47f-4345-b3ec-dbeaecadb5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, labels]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-389a6704-9c3d-4b9c-9a8a-d46c4c9ff65d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-389a6704-9c3d-4b9c-9a8a-d46c4c9ff65d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-389a6704-9c3d-4b9c-9a8a-d46c4c9ff65d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-389a6704-9c3d-4b9c-9a8a-d46c4c9ff65d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf generator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# For only bigrams\n",
        "#bigram_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
        "#bigram_matrix = bigram_vectorizer.fit_transform(data)\n",
        "#dense_matrix = bigram_matrix.todense()\n",
        "\n",
        "# Get feature names to see which bigrams are in the vocabulary\n",
        "#feature_names = bigram_vectorizer.get_feature_names_out()\n",
        "#bigram_df = pd.DataFrame(bigram_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# For both unigrams and bigrams\n",
        "ngram_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "ngram_matrix = ngram_vectorizer.fit_transform(data)\n",
        "ngram_dense_matrix = ngram_matrix.todense()\n",
        "\n",
        "# Get feature names to see which bigrams are in the vocabulary\n",
        "feature_names = ngram_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display the matrix or feature names\n",
        "# print(dense_matrix)\n",
        "#print(len(feature_names))\n",
        "ngram_df = pd.DataFrame(ngram_matrix.toarray(), columns=feature_names)"
      ],
      "metadata": {
        "id": "iwqd4jIvh3NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_df"
      ],
      "metadata": {
        "id": "l7UumpRrRCoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top K Ngrams as features using RandomForest clf"
      ],
      "metadata": {
        "id": "y3yh5aLK5sla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(ngram_matrix, labels_array)  # 'labels' is the array of sentiment labels\n",
        "\n",
        "importances = clf.feature_importances_"
      ],
      "metadata": {
        "id": "-9qb63eh0lVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Get indices of sorted importances\n",
        "sorted_indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Select the top N feature indices\n",
        "N = 1000  # For example, select the top 1000 features\n",
        "top_feature_indices = sorted_indices[:N]\n",
        "\n",
        "# Select the top N features from the original feature matrix\n",
        "X_top_ngrams = ngram_matrix[:, top_feature_indices]"
      ],
      "metadata": {
        "id": "U6l1DrQR1K8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER experimentation"
      ],
      "metadata": {
        "id": "3q8r0uF0JR9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy transformers"
      ],
      "metadata": {
        "id": "hV9xy6fNNgs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try word capitalization to improve results (failed)\n",
        "#df['title'] = df['title'].str.lower().apply(string.capwords)"
      ],
      "metadata": {
        "id": "WdjTrAvI6brx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import spacy, string\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    return organizations\n",
        "\n",
        "df['NER'] = df['title'].apply(perform_ner)\n",
        "\n",
        "def refine_org_name(ner_results):\n",
        "    orgs_list = []\n",
        "    org_name = \"\"\n",
        "    # A set of common words not typically part of an organization name\n",
        "    non_org_keywords = {'reports', 'results', 'quarter', 'earnings', 'profit', 'loss', 'revenue', 'confirms', 'increase', 'delivers'}\n",
        "\n",
        "    for result in ner_results:\n",
        "\n",
        "        entity = result['entity']\n",
        "        word = result['word']\n",
        "\n",
        "        # check for first org occurence\n",
        "        if entity == 'B-ORG':\n",
        "          #add prev org to list\n",
        "          if len(org_name) > 0:\n",
        "            orgs_list.append(org_name.strip())\n",
        "          org_name = word\n",
        "        # check for I org subword\n",
        "        elif entity == 'I-ORG':\n",
        "          if word[0:2] == \"##\":\n",
        "            word = word.replace('##', '')\n",
        "            org_name += word\n",
        "        # check for I org but not subword\n",
        "          elif(word.lower() not in non_org_keywords):\n",
        "            org_name += f\" {word}\"\n",
        "\n",
        "    if org_name.strip() not in orgs_list and org_name:\n",
        "      orgs_list.append(org_name.strip())\n",
        "\n",
        "    return orgs_list\n",
        "\n",
        "\n",
        "# Use the refined function to reconstruct the organization name\n",
        "def BERT_NER(dataset):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
        "  model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
        "  nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "  NER_res = []\n",
        "\n",
        "  for test in tests:\n",
        "    NER_res.append(refine_org_name(nlp(test)))\n",
        "\n",
        "  return NER_res\n"
      ],
      "metadata": {
        "id": "Csv4xCicXjnQ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_results = df.NER[0:50].tolist()\n",
        "spacy_results"
      ],
      "metadata": {
        "id": "boK6VWfcZQ2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a5aa41-f7f3-4062-af2d-21086d95f158"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " ['TRILLION ENERGY ANNOUNCES FLOW TEST RESULTS FOR WELL'],\n",
              " [],\n",
              " ['Unigold Inc Delivers Positive Feasibility Study for Candelones Oxide Project'],\n",
              " [],\n",
              " ['Ascendant Resources Reports Copper Intercept', 'Lagoa Salgada'],\n",
              " ['Barrick'],\n",
              " ['TomaGold'],\n",
              " ['Surge Energy Inc Announces',\n",
              "  'Financings Anticipated Dividend Increase Increased Production Rate'],\n",
              " ['Stack Capital Group Inc'],\n",
              " ['Diversified Royalty Corp Completes Acquisition', 'Confirms Increase'],\n",
              " ['Visionary Education Technology', 'LOI', 'Acquire Equity Interest'],\n",
              " ['Xenon Pharmaceuticals Announces Conference Call and Webcast to Discuss Third Quarter Financial Results'],\n",
              " ['InMed Pharmaceuticals Advances Neurodegenerative Disease Program',\n",
              "  'Natural Sciences and Engineering Research Council of Canada NSERC Alliance'],\n",
              " ['Acquire Additional Mining', 'Exclusive Prospective Licences'],\n",
              " ['Canadian Life Companies Split Corp Preferred'],\n",
              " [],\n",
              " ['Ceapro Inc Launches Next Phase', 'Disruptive PGX Technology'],\n",
              " ['TELUS International'],\n",
              " [],\n",
              " [],\n",
              " ['Reunion Gold Announces Appointment'],\n",
              " ['Premier Health Announces TSXV Approval'],\n",
              " [],\n",
              " ['Gold Bull Announces Filing', 'PEA Technical Report'],\n",
              " ['United Hunter Oil Gas Corp Announces', 'Bocana Resources Ltd'],\n",
              " [],\n",
              " ['Multiple Solid Tumors', 'Tissue'],\n",
              " ['Manitoba Liquor Lotteries'],\n",
              " [],\n",
              " ['Total Energy Services Inc'],\n",
              " ['New Porphyry Lithocap Target El Cobre Project'],\n",
              " ['Procurated'],\n",
              " ['K Mining Announces'],\n",
              " ['TSX VENTURE'],\n",
              " ['Prairie Provident Resources Announces Decision'],\n",
              " ['Bunker Hill Announces Marketed Public Offering'],\n",
              " ['Caledonia Mining Corporation Plc'],\n",
              " ['UO Corp Announces Name Change to Green Shift Commodities Ltd'],\n",
              " ['PyroGenesis Announces Participation',\n",
              "  'Major International Aluminum Company'],\n",
              " ['Pricing Update'],\n",
              " ['The Howard Group Inc', 'Announces Stock Option Grants'],\n",
              " [],\n",
              " ['Ukraine Government Military Service'],\n",
              " ['Labrador Gold Intersects'],\n",
              " [],\n",
              " [],\n",
              " ['Psyence Group Announces'],\n",
              " ['General Magnesium Corporation'],\n",
              " ['Bombardier to Report']]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 34s for 50 entries, 40 mins for 5000\n",
        "tests = df.title[0:50].tolist()\n",
        "bert_results = BERT_NER(tests)\n",
        "bert_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFaazUMD0538",
        "outputId": "0003f1a5-9733-4fa2-f09a-9602fe3b165d"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# captures orgs not in bert but in spacy but spacy inaccurate sometimes (useless)\n",
        "def combine_results(spacy, bert):\n",
        "\n",
        "    combined_results = []\n",
        "    for orgs_spacy, orgs_bert in zip(spacy, bert):\n",
        "        #check both items are empty or not\n",
        "        if orgs_spacy or orgs_bert:\n",
        "\n",
        "            #check if either is empty\n",
        "            if len(orgs_spacy) == 0 and orgs_bert:\n",
        "              combined_results.append(orgs_bert)\n",
        "            elif len(orgs_bert) == 0 and orgs_spacy:\n",
        "              combined_results.append(orgs_spacy)\n",
        "            else:\n",
        "              #favour bert so set bert as default\n",
        "              result = orgs_bert\n",
        "\n",
        "              # check for any org overlap\n",
        "              for org_spacy in orgs_spacy:\n",
        "                overlap = False\n",
        "\n",
        "                if org_spacy in orgs_bert:\n",
        "                  # dont add this org\n",
        "                  continue\n",
        "\n",
        "                else:\n",
        "                  for subword_org_spacy in org_spacy.split():\n",
        "                    if subword_org_spacy in ' '.join(orgs_bert).split():\n",
        "                      overlap = True\n",
        "                      break\n",
        "                  if not overlap:\n",
        "                    result.append(org_spacy)\n",
        "              combined_results.append(result)\n",
        "        else:\n",
        "          combined_results.append('No Entity Recognised')\n",
        "\n",
        "    # Return the combined list of results\n",
        "    return combined_results\n",
        "\n",
        "test1 = combine_results(spacy_results,bert_results)\n",
        "\n",
        "def combine_results(spacy, bert):\n",
        "    combined_results = []\n",
        "    for orgs_spacy, orgs_bert in zip(spacy, bert):\n",
        "      if orgs_bert:\n",
        "        combined_results.append(orgs_bert)\n",
        "      elif orgs_spacy:\n",
        "        combined_results.append(orgs_spacy)\n",
        "      else:\n",
        "        combined_results.append('No Entity Recognised')\n",
        "    return combined_results\n",
        "\n",
        "test2 = combine_results(spacy_results, bert_results)\n",
        "\n",
        "for result1, result2 in zip(test1, test2):\n",
        "  print(result1)\n",
        "  print(result2)\n",
        "  print('----\\n')\n"
      ],
      "metadata": {
        "id": "YVz3UwBUhEqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL NER CODE TO USE"
      ],
      "metadata": {
        "id": "UsCTjrwTHmbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "ntk79Qy7Hysg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('try_cleaned_data.csv')\n",
        "df = df.drop(columns=df.columns[0], axis=1)\n",
        "df[df.isnull().any(axis=1)]\n",
        "df.dropna(subset=['title'], inplace=True)\n",
        "data= df['title'].tolist()\n",
        "labels_df = df[['labels']]\n",
        "labels_array = df['labels'].tolist()"
      ],
      "metadata": {
        "id": "JlscgIUd8RaQ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPACY NER"
      ],
      "metadata": {
        "id": "ddqnU7UAH0e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slice and tried using top 200 rows\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def perform_ner(text):\n",
        "    doc = nlp(text)\n",
        "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    return organizations\n",
        "\n",
        "df['NER'] = df['title'].apply(perform_ner)\n",
        "spacy_results = df.NER[0:200].tolist()\n",
        "spacy_results"
      ],
      "metadata": {
        "id": "t3t9wqCPH3Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT NER"
      ],
      "metadata": {
        "id": "ZP7AgAprH88J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# slice and tried using top 200 rows\n",
        "def refine_org_name(ner_results):\n",
        "    orgs_list = []\n",
        "    org_name = \"\"\n",
        "    # A set of common words not typically part of an organization name\n",
        "    non_org_keywords = {'reports', 'results', 'quarter', 'earnings', 'profit', 'loss', 'revenue', 'confirms', 'increase', 'delivers'}\n",
        "\n",
        "    for result in ner_results:\n",
        "\n",
        "        entity = result['entity']\n",
        "        word = result['word']\n",
        "\n",
        "        # check for first org occurence\n",
        "        subword_flag = False\n",
        "        if entity == 'B-ORG':\n",
        "          # check if subword\n",
        "          if word[0:2] == \"##\":\n",
        "            word = word.replace('##', '')\n",
        "            org_name += word\n",
        "            subword_flag = True\n",
        "          else:\n",
        "            #add prev org to list\n",
        "            if len(org_name) > 0:\n",
        "              orgs_list.append(org_name.strip())\n",
        "            org_name = word\n",
        "        # check for I org subword\n",
        "        elif entity == 'I-ORG':\n",
        "          if word[0:2] == \"##\":\n",
        "            word = word.replace('##', '')\n",
        "            org_name += word\n",
        "        # check for I org but not subword\n",
        "          elif(word.lower() not in non_org_keywords):\n",
        "            org_name += f\" {word}\"\n",
        "\n",
        "    if org_name.strip() not in orgs_list and org_name:\n",
        "      orgs_list.append(org_name.strip())\n",
        "\n",
        "    return orgs_list\n",
        "\n",
        "\n",
        "# Use the refined function to reconstruct the organization name\n",
        "def BERT_NER(dataset):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"butchland/bert-finetuned-ner\")\n",
        "  model = AutoModelForTokenClassification.from_pretrained(\"butchland/bert-finetuned-ner\")\n",
        "  nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0 )\n",
        "  NER_res = []\n",
        "\n",
        "  for sentence in dataset:\n",
        "    sentence_orgs = refine_org_name(nlp(sentence))\n",
        "    # one more layer of check if ORG name not in original sentence\n",
        "    # handling unexpected orgs like dividend -> dind\n",
        "    # for org in sentence_orgs:\n",
        "    #   for word in org.split():\n",
        "    #     if word not in sentence.split():\n",
        "    #       print(word)\n",
        "    #       print(sentence)\n",
        "    #       print(sentence_orgs)\n",
        "    #       sentence_orgs.remove(org)\n",
        "    NER_res.append(sentence_orgs)\n",
        "  return NER_res\n",
        "\n",
        "tests = df.title[0:200].tolist()\n",
        "bert_results = BERT_NER(tests)\n",
        "bert_results\n"
      ],
      "metadata": {
        "id": "CqLsEvlOH-z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combine SPACY and BERT NER Results"
      ],
      "metadata": {
        "id": "YtXO1BHVIRMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_results(spacy, bert):\n",
        "    combined_results = []\n",
        "    for orgs_spacy, orgs_bert in zip(spacy, bert):\n",
        "      if orgs_bert:\n",
        "        combined_results.append(orgs_bert)\n",
        "      elif orgs_spacy:\n",
        "        combined_results.append(orgs_spacy)\n",
        "      else:\n",
        "        combined_results.append('No Entity Recognised')\n",
        "    return combined_results\n",
        "\n",
        "NER_col_results = combine_results(spacy_results, bert_results)\n",
        "# for i in range(13684-50):\n",
        "#   NER_col_results.append('NA')\n",
        "#df = df.loc[0:199]\n",
        "df['Entities'] = NER_col_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7SXcwXxIS2g",
        "outputId": "c4ec05bf-19df-4d3f-82ec-56706f20b6ba"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-139-4999c462c043>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Entities'] = NER_col_results\n"
          ]
        }
      ]
    }
  ]
}