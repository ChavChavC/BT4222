{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChavChavC/BT4222/blob/main/Feature_Engineering1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K4NRlWdlC83w"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = load_dataset(\"Jean-Baptiste/financial_news_sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "Mvrf-oNZIE_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example text."
      ],
      "metadata": {
        "id": "GMGO6oW6DEOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models to do:\n",
        "\n",
        "1.   DistilBERT\n",
        "2.   BERT\n",
        "3.   FinBERT\n",
        "4.   SVM\n",
        "5.   RF\n",
        "6.   Roberta-fin (their model) lmao"
      ],
      "metadata": {
        "id": "mXVPegbO4eTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROBERTA (their model)"
      ],
      "metadata": {
        "id": "hnEr_S7h3x-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBERTA FIN BERT model\n",
        "#Their own model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Jean-Baptiste/roberta-large-financial-news-sentiment-en\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-financial-news-sentiment-en\")\n",
        "\n",
        "classifier=pipeline(\"text-classification\",model=model, tokenizer=tokenizer)\n",
        "output=classifier(dataset['train']['summary_detail_with_title'][0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdwnk0J5ocEx",
        "outputId": "c6abf16e-864b-4772-a562-b4cf47f408f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'positive', 'score': 0.9405916333198547}, {'label': 'neutral', 'score': 0.9450716376304626}, {'label': 'positive', 'score': 0.9427127838134766}, {'label': 'positive', 'score': 0.9426810145378113}, {'label': 'positive', 'score': 0.9421485662460327}, {'label': 'neutral', 'score': 0.9456402063369751}, {'label': 'positive', 'score': 0.9421244263648987}, {'label': 'neutral', 'score': 0.9445323348045349}, {'label': 'neutral', 'score': 0.9450904726982117}, {'label': 'positive', 'score': 0.9429874420166016}, {'label': 'neutral', 'score': 0.9449349641799927}, {'label': 'neutral', 'score': 0.945558488368988}, {'label': 'positive', 'score': 0.9414768815040588}, {'label': 'neutral', 'score': 0.9459429383277893}, {'label': 'positive', 'score': 0.9428868889808655}, {'label': 'positive', 'score': 0.9431929588317871}, {'label': 'neutral', 'score': 0.9440200924873352}, {'label': 'neutral', 'score': 0.9446520805358887}, {'label': 'neutral', 'score': 0.94461989402771}, {'label': 'positive', 'score': 0.9435655474662781}, {'label': 'positive', 'score': 0.9439270496368408}, {'label': 'positive', 'score': 0.9434192180633545}, {'label': 'neutral', 'score': 0.9464735388755798}, {'label': 'neutral', 'score': 0.9444798231124878}, {'label': 'neutral', 'score': 0.9438926577568054}, {'label': 'neutral', 'score': 0.9434136748313904}, {'label': 'positive', 'score': 0.9427061080932617}, {'label': 'positive', 'score': 0.9431478381156921}, {'label': 'positive', 'score': 0.9410275220870972}, {'label': 'positive', 'score': 0.9422452449798584}, {'label': 'positive', 'score': 0.9415075182914734}, {'label': 'neutral', 'score': 0.944858729839325}, {'label': 'positive', 'score': 0.943261981010437}, {'label': 'neutral', 'score': 0.9425995349884033}, {'label': 'neutral', 'score': 0.9442760944366455}, {'label': 'neutral', 'score': 0.9454383850097656}, {'label': 'positive', 'score': 0.9422639012336731}, {'label': 'neutral', 'score': 0.9458196759223938}, {'label': 'neutral', 'score': 0.9442287087440491}, {'label': 'neutral', 'score': 0.946535587310791}, {'label': 'positive', 'score': 0.9362429976463318}, {'label': 'positive', 'score': 0.942449152469635}, {'label': 'positive', 'score': 0.9415122866630554}, {'label': 'neutral', 'score': 0.9446828365325928}, {'label': 'neutral', 'score': 0.9454922676086426}, {'label': 'neutral', 'score': 0.9453178644180298}, {'label': 'neutral', 'score': 0.943743109703064}, {'label': 'neutral', 'score': 0.9457515478134155}, {'label': 'positive', 'score': 0.9428964853286743}, {'label': 'negative', 'score': 0.9383538961410522}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBERT / assuming BERT will be similar just that we changing input of 'from_pretrained()' (?)"
      ],
      "metadata": {
        "id": "UYEN-WkU3zC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distilbert\n",
        "from transformers import AutoTokenizer, DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"summary_detail_with_title\"], truncation=True)\n",
        "tokenizeddataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2:\"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"NEUTRAL\":1 , \"POSITIVE\": 2}\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenizeddataset[\"train\"],\n",
        "    eval_dataset=tokenizeddataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "dv0FaC3fon4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=pipeline(\"text-classification\",model=model, tokenizer=tokenizer)\n",
        "output=classifier(dataset['train']['summary_detail_with_title'][0])\n",
        "print(output)"
      ],
      "metadata": {
        "id": "x_wqbOA8yJq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}